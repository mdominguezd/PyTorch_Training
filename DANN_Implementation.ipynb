{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ba30470-4b9f-4873-a1c4-93fe657548cf",
      "metadata": {
        "id": "6ba30470-4b9f-4873-a1c4-93fe657548cf"
      },
      "source": [
        "# params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "4ff18998-d388-4514-9b84-79a771ccc62e",
      "metadata": {
        "id": "4ff18998-d388-4514-9b84-79a771ccc62e"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 10\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b04138-eca8-4e5b-9ef7-305ed7133475",
      "metadata": {
        "id": "67b04138-eca8-4e5b-9ef7-305ed7133475"
      },
      "source": [
        "# mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "aee47c53-0a37-493d-bcb8-a259b4bc2823",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "aee47c53-0a37-493d-bcb8-a259b4bc2823"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))\n",
        "                                ])\n",
        "\n",
        "mnist_train_dataset = datasets.MNIST(root='data/pytorch/MNIST', train=True, download=True,\n",
        "                                     transform=transform)\n",
        "mnist_valid_dataset = datasets.MNIST(root='data/pytorch/MNIST', train=True, download=True,\n",
        "                                     transform=transforms)\n",
        "mnist_test_dataset = datasets.MNIST(root='data/pytorch/MNIST', train=False, transform=transform)\n",
        "\n",
        "indices = list(range(len(mnist_train_dataset)))\n",
        "validation_size = 5000\n",
        "train_idx, valid_idx = indices[validation_size:], indices[:validation_size]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "mnist_train_loader = DataLoader(\n",
        "    mnist_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnist_valid_loader = DataLoader(\n",
        "    mnist_valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnist_test_loader = DataLoader(\n",
        "    mnist_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "\n",
        "# mnist_train_all = (mnist_train_dataset.train_data[5000:].reshape(55000, 28, 28, 1))\n",
        "# mnist_concat = torch.cat((mnist_train_all, mnist_train_all, mnist_train_all), 3)\n",
        "# print(mnist_test_dataset.test_labels.shape, mnist_test_dataset.test_labels)\n",
        "\n",
        "\n",
        "def one_hot_embedding(labels, num_classes=10):\n",
        "    \"\"\"Embedding labels to one-hot form.\n",
        "\n",
        "    Args:\n",
        "      labels: (LongTensor) class labels, sized [N,].\n",
        "      num_classes: (int) number of classes.\n",
        "\n",
        "    Returns:\n",
        "      (tensor) encoded labels, sized [N, #classes].\n",
        "    \"\"\"\n",
        "    y = torch.eye(num_classes)\n",
        "    return y[labels]\n",
        "\n",
        "\n",
        "# print(one_hot_embedding(mnist_test_dataset.test_labels))\n",
        "\n",
        "# print(mnist_concat.shape)\n",
        "\n",
        "\n",
        "# def test():\n",
        "    # print(mnist_train_loader.shape)\n",
        "    # print(len(train_sampler), len(mnist_test_loader), len(valid_sampler))\n",
        "    # print(len(mnist_train_loader), len(mnist_valid_loader), len(mnist_test_loader))\n",
        "    # for i, train_data in enumerate(mnist_train_loader):\n",
        "    #     img, label = train_data\n",
        "    #     print(img.shape)\n",
        "    # for i in range(1):\n",
        "    #     # for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "    #     #     print(i, batch_idx, labels, len(labels))\n",
        "    # mnist_train_all = (mnist_train_dataset.train_data[5000:].reshape(55000, 28, 28, 1))\n",
        "    # mnist_concat = torch.cat((mnist_train_all, mnist_train_all, mnist_train_all), 3)\n",
        "    # print(mnist_concat.shape)\n",
        "    # print(list(mnist_train_dataset.train_data[5000:].size()))\n",
        "    # print(mnist_train_dataset.train_data.float().mean()/255)\n",
        "    # print(mnist_train_dataset.train_data.float().std()/255)\n",
        "    # for batch_idx, (train_data, test_data) in enumerate(zip(mnist_train_loader, mnist_valid_loader)):\n",
        "    #     train_image, train_label = train_data\n",
        "    #     test_image, test_label = test_data\n",
        "    #     print(train_image.shape)\n",
        "    #     # print(train_label, len(train_label))\n",
        "    #     # print(test_label, len(test_label))\n",
        "    #     # exit()\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "907bd09d-8ada-4aa6-9857-a4422dd98e07",
      "metadata": {
        "id": "907bd09d-8ada-4aa6-9857-a4422dd98e07"
      },
      "source": [
        "# mnistm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3a33dbfe-2aaf-4fac-bb9e-89407fb7c167",
      "metadata": {
        "id": "3a33dbfe-2aaf-4fac-bb9e-89407fb7c167"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import os\n",
        "import errno\n",
        "from PIL import Image\n",
        "\n",
        "# MNIST-M\n",
        "class MNISTM(data.Dataset):\n",
        "    \"\"\"`MNIST-M Dataset.\"\"\"\n",
        "\n",
        "    url = \"https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz\"\n",
        "\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'processed'\n",
        "    training_file = 'mnist_m_train.pt'\n",
        "    test_file = 'mnist_m_test.pt'\n",
        "\n",
        "    def __init__(self,\n",
        "                 root, mnist_root=\"data\",\n",
        "                 train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        \"\"\"Init MNIST-M dataset.\"\"\"\n",
        "        super(MNISTM, self).__init__()\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.mnist_root = os.path.expanduser(mnist_root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        if self.train:\n",
        "            self.train_data, self.train_labels = \\\n",
        "                torch.load(os.path.join(self.root,\n",
        "                                        self.processed_folder,\n",
        "                                        self.training_file))\n",
        "        else:\n",
        "            self.test_data, self.test_labels = \\\n",
        "                torch.load(os.path.join(self.root,\n",
        "                                        self.processed_folder,\n",
        "                                        self.test_file))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get images and target for data loader.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            img, target = self.train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            img, target = self.test_data[index], self.test_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        # print(type(img))\n",
        "        img = Image.fromarray(img.squeeze().numpy(), mode='RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return size of dataset.\"\"\"\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root,\n",
        "                                           self.processed_folder,\n",
        "                                           self.training_file)) and \\\n",
        "               os.path.exists(os.path.join(self.root,\n",
        "                                           self.processed_folder,\n",
        "                                           self.test_file))\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the MNIST data.\"\"\"\n",
        "        # import essential packages\n",
        "        from six.moves import urllib\n",
        "        import gzip\n",
        "        import pickle\n",
        "        from torchvision import datasets\n",
        "\n",
        "        # check if dataset already exists\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        # make data dirs\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        # download pkl files\n",
        "        print('Downloading ' + self.url)\n",
        "        filename = self.url.rpartition('/')[2]\n",
        "        file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "        if not os.path.exists(file_path.replace('.gz', '')):\n",
        "            data = urllib.request.urlopen(self.url)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
        "                    gzip.GzipFile(file_path) as zip_f:\n",
        "                out_f.write(zip_f.read())\n",
        "            os.unlink(file_path)\n",
        "\n",
        "        # process and save as torch files\n",
        "        print('Processing...')\n",
        "\n",
        "        # load MNIST-M images from pkl file\n",
        "        with open(file_path.replace('.gz', ''), \"rb\") as f:\n",
        "            mnist_m_data = pickle.load(f, encoding='bytes')\n",
        "        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b'train'])\n",
        "        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b'test'])\n",
        "\n",
        "        # get MNIST labels\n",
        "        mnist_train_labels = datasets.MNIST(root=self.mnist_root,\n",
        "                                            train=True,\n",
        "                                            download=True).train_labels\n",
        "        mnist_test_labels = datasets.MNIST(root=self.mnist_root,\n",
        "                                           train=False,\n",
        "                                           download=True).test_labels\n",
        "\n",
        "        # save MNIST-M dataset\n",
        "        training_set = (mnist_m_train_data, mnist_train_labels)\n",
        "        test_set = (mnist_m_test_data, mnist_test_labels)\n",
        "        with open(os.path.join(self.root,\n",
        "                               self.processed_folder,\n",
        "                               self.training_file), 'wb') as f:\n",
        "            torch.save(training_set, f)\n",
        "        with open(os.path.join(self.root,\n",
        "                               self.processed_folder,\n",
        "                               self.test_file), 'wb') as f:\n",
        "            torch.save(test_set, f)\n",
        "\n",
        "        print('MNISTM Done!')\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.29730626, 0.29918741, 0.27534935),\n",
        "                                                     (0.32780124, 0.32292358, 0.32056796))\n",
        "                                ])\n",
        "\n",
        "mnistm_train_dataset = MNISTM(root='data/pytorch/MNIST-M', train=True, download=True,\n",
        "                              transform=transform)\n",
        "mnistm_valid_dataset = MNISTM(root='data/pytorch/MNIST-M', train=True, download=True,\n",
        "                              transform=transform)\n",
        "mnistm_test_dataset = MNISTM(root='data/pytorch/MNIST-M', train=False, transform=transform)\n",
        "\n",
        "indices = list(range(len(mnistm_train_dataset)))\n",
        "validation_size = 5000\n",
        "train_idx, valid_idx = indices[validation_size:], indices[:validation_size]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "mnistm_train_loader = DataLoader(\n",
        "    mnistm_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnistm_valid_loader = DataLoader(\n",
        "    mnistm_valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnistm_test_loader = DataLoader(\n",
        "    mnistm_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "# print(mnistm_train_dataset.train_data[5000:].shape)\n",
        "# mnistm_concat = (mnistm_train_dataset.train_data[5000:])\n",
        "\n",
        "\n",
        "# def test():\n",
        "#     print(mnistm_train_dataset.train_data[5000:].shape)\n",
        "#     print((mnistm_train_dataset.train_data[5000:].size()))\n",
        "#\n",
        "#     print(len(train_sampler), len(mnistm_test_loader), len(valid_sampler))\n",
        "#     print(len(mnistm_train_loader), len(mnistm_valid_loader), len(mnistm_test_loader))\n",
        "#     for i in range(1):\n",
        "#         for batch_idx, (inputs, labels) in enumerate(mnistm_train_loader):\n",
        "#             print(i, batch_idx, labels, len(labels))\n",
        "#     for batch_idx, (train_data, test_data) in enumerate(zip(mnistm_train_loader, mnistm_valid_loader)):\n",
        "#         train_image, train_label = train_data\n",
        "#         test_image, test_label = test_data\n",
        "#         print(train_label, len(train_label))\n",
        "#         print(test_label, len(test_label))\n",
        "#         exit()\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ecc02f5-7800-4d8f-ba7e-a36e4ef824e8",
      "metadata": {
        "id": "8ecc02f5-7800-4d8f-ba7e-a36e4ef824e8"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "25ecc5f5-55e6-4eca-b11d-417d83b5d138",
      "metadata": {
        "id": "25ecc5f5-55e6-4eca-b11d-417d83b5d138"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Function\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "\n",
        "def optimizer_scheduler(optimizer, p):\n",
        "    \"\"\"\n",
        "    Adjust the learning rate of optimizer\n",
        "    :param optimizer: optimizer for updating parameters\n",
        "    :param p: a variable for adjusting learning rate\n",
        "    :return: optimizer\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def one_hot_embedding(labels, num_classes=10):\n",
        "    \"\"\"Embedding labels to one-hot form.\n",
        "\n",
        "    Args:\n",
        "      labels: (LongTensor) class labels, sized [N,].\n",
        "      num_classes: (int) number of classes.\n",
        "\n",
        "    Returns:\n",
        "      (tensor) encoded labels, sized [N, #classes].\n",
        "    \"\"\"\n",
        "    y = torch.eye(num_classes)\n",
        "    return y[labels]\n",
        "\n",
        "\n",
        "def save_model(encoder, classifier, discriminator, training_mode, save_name):\n",
        "    print('Save models ...')\n",
        "\n",
        "    save_folder = 'trained_models'\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    torch.save(encoder.state_dict(), 'trained_models/encoder_' + str(training_mode) + '_' + str(save_name) + '.pt')\n",
        "    torch.save(classifier.state_dict(), 'trained_models/classifier_' + str(training_mode) + '_' + str(save_name) + '.pt')\n",
        "\n",
        "    if training_mode == 'dann':\n",
        "        torch.save(discriminator.state_dict(), 'trained_models/discriminator_' + str(training_mode) + '_' + str(save_name) + '.pt')\n",
        "\n",
        "    print('Model is saved !!!')\n",
        "\n",
        "\n",
        "def plot_embedding(X, y, d, training_mode, save_name):\n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "    y = list(itertools.chain.from_iterable(y))\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(len(d)):  # X.shape[0] : 1024\n",
        "        # plot colored number\n",
        "        if d[i] == 0:\n",
        "            colors = (0.0, 0.0, 1.0, 1.0)\n",
        "        else:\n",
        "            colors = (1.0, 0.0, 0.0, 1.0)\n",
        "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                 color=colors,\n",
        "                 fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    if save_name is not None:\n",
        "        plt.title(save_name)\n",
        "\n",
        "    save_folder = 'saved_plot'\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    fig_name = 'saved_plot/' + str(training_mode) + '_' + str(save_name) + '.png'\n",
        "    plt.savefig(fig_name)\n",
        "    print('{} is saved'.format(fig_name))\n",
        "\n",
        "\n",
        "def visualize(encoder, training_mode, save_name):\n",
        "    # Draw 512 samples in test_data\n",
        "    source_test_loader = mnist_test_loader\n",
        "    target_test_loader = mnistm_test_loader\n",
        "\n",
        "    # Get source_test samples\n",
        "    source_label_list = []\n",
        "    source_img_list = []\n",
        "    for i, test_data in enumerate(source_test_loader):\n",
        "        if i >= 16:  # to get only 512 samples\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        img = img.cuda()\n",
        "        img = torch.cat((img, img, img), 1)  # MNIST channel 1 -> 3\n",
        "        source_label_list.append(label)\n",
        "        source_img_list.append(img)\n",
        "\n",
        "    source_img_list = torch.stack(source_img_list)\n",
        "    source_img_list = source_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Get target_test samples\n",
        "    target_label_list = []\n",
        "    target_img_list = []\n",
        "    for i, test_data in enumerate(target_test_loader):\n",
        "        if i >= 16:\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        img = img.cuda()\n",
        "        target_label_list.append(label)\n",
        "        target_img_list.append(img)\n",
        "\n",
        "    target_img_list = torch.stack(target_img_list)\n",
        "    target_img_list = target_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Stack source_list + target_list\n",
        "    combined_label_list = source_label_list\n",
        "    combined_label_list.extend(target_label_list)\n",
        "    combined_img_list = torch.cat((source_img_list, target_img_list), 0)\n",
        "\n",
        "    source_domain_list = torch.zeros(512).type(torch.LongTensor)\n",
        "    target_domain_list = torch.ones(512).type(torch.LongTensor)\n",
        "    combined_domain_list = torch.cat((source_domain_list, target_domain_list), 0).cuda()\n",
        "\n",
        "    print(\"Extract features to draw T-SNE plot...\")\n",
        "    combined_feature = encoder(combined_img_list)  # combined_feature : 1024,2352\n",
        "\n",
        "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
        "    dann_tsne = tsne.fit_transform(combined_feature.detach().cpu().numpy())\n",
        "\n",
        "    print('Draw plot ...')\n",
        "    save_name = save_name + '_' + str(training_mode)\n",
        "    plot_embedding(dann_tsne, combined_label_list, combined_domain_list, training_mode, save_name)\n",
        "\n",
        "\n",
        "def visualize_input():\n",
        "    source_test_loader = mnist_test_loader\n",
        "    target_test_loader = mnistm_test_loader\n",
        "\n",
        "    # Get source_test samples\n",
        "    source_label_list = []\n",
        "    source_img_list = []\n",
        "    for i, test_data in enumerate(source_test_loader):\n",
        "        if i >= 16:  # to get only 512 samples\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        img = img.cuda()\n",
        "        img = torch.cat((img, img, img), 1)  # MNIST channel 1 -> 3\n",
        "        source_label_list.append(label)\n",
        "        source_img_list.append(img)\n",
        "\n",
        "    source_img_list = torch.stack(source_img_list)\n",
        "    source_img_list = source_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Get target_test samples\n",
        "    target_label_list = []\n",
        "    target_img_list = []\n",
        "    for i, test_data in enumerate(target_test_loader):\n",
        "        if i >= 16:\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        img = img.cuda()\n",
        "        target_label_list.append(label)\n",
        "        target_img_list.append(img)\n",
        "\n",
        "    target_img_list = torch.stack(target_img_list)\n",
        "    target_img_list = target_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Stack source_list + target_list\n",
        "    combined_label_list = source_label_list\n",
        "    combined_label_list.extend(target_label_list)\n",
        "    combined_img_list = torch.cat((source_img_list, target_img_list), 0)\n",
        "\n",
        "    source_domain_list = torch.zeros(512).type(torch.LongTensor)\n",
        "    target_domain_list = torch.ones(512).type(torch.LongTensor)\n",
        "    combined_domain_list = torch.cat((source_domain_list, target_domain_list), 0).cuda()\n",
        "\n",
        "    print(\"Extract features to draw T-SNE plot...\")\n",
        "    combined_feature = combined_img_list  # combined_feature : 1024,3,28,28\n",
        "    combined_feature = combined_feature.view(1024, -1)  # flatten\n",
        "    # print(type(combined_feature), combined_feature.shape)\n",
        "\n",
        "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
        "    dann_tsne = tsne.fit_transform(combined_feature.detach().cpu().numpy())\n",
        "    print('Draw plot ...')\n",
        "    save_name = 'input_tsne_plot'\n",
        "    plot_embedding(dann_tsne, combined_label_list, combined_domain_list, 'input', 'mnist_n_mnistM')\n",
        "\n",
        "\n",
        "def get_free_gpu():\n",
        "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
        "    # memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
        "    return 0\n",
        "\n",
        "def set_model_mode(mode='train', models=None):\n",
        "    for model in models:\n",
        "        if mode == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e307273e-5ea5-4592-8940-2a3612ffd22c",
      "metadata": {
        "id": "e307273e-5ea5-4592-8940-2a3612ffd22c"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "3a9ad254-19d0-46fe-adc4-49c287d906f5",
      "metadata": {
        "id": "3a9ad254-19d0-46fe-adc4-49c287d906f5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Extractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Extractor, self).__init__()\n",
        "        self.extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=48, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extractor(x)\n",
        "        x = x.view(-1, 3 * 28 * 28)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=3 * 28 * 28, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return F.softmax(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(in_features=3 * 28 * 28, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_feature, alpha):\n",
        "        reversed_input = ReverseLayerF.apply(input_feature, alpha)\n",
        "        x = self.discriminator(reversed_input)\n",
        "        return F.softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b6c2d5-7a9e-4484-8894-950b5d14e2c6",
      "metadata": {
        "id": "d7b6c2d5-7a9e-4484-8894-950b5d14e2c6"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "af4b18ea-8719-465d-b346-435b798bc550",
      "metadata": {
        "id": "af4b18ea-8719-465d-b346-435b798bc550"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode):\n",
        "    print(\"Model test ...\")\n",
        "\n",
        "    encoder.cuda()\n",
        "    classifier.cuda()\n",
        "    set_model_mode('eval', [encoder, classifier])\n",
        "\n",
        "    if training_mode == 'dann':\n",
        "        discriminator.cuda()\n",
        "        set_model_mode('eval', [discriminator])\n",
        "        domain_correct = 0\n",
        "\n",
        "    source_correct = 0\n",
        "    target_correct = 0\n",
        "\n",
        "    for batch_idx, (source_data, target_data) in enumerate(zip(source_test_loader, target_test_loader)):\n",
        "        p = float(batch_idx) / len(source_test_loader)\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "        # 1. Source input -> Source Classification\n",
        "        source_image, source_label = source_data\n",
        "        source_image, source_label = source_image.cuda(), source_label.cuda()\n",
        "        source_image = torch.cat((source_image, source_image, source_image), 1)  # MNIST convert to 3 channel\n",
        "        source_feature = encoder(source_image)\n",
        "        source_output = classifier(source_feature)\n",
        "        source_pred = source_output.data.max(1, keepdim=True)[1]\n",
        "        source_correct += source_pred.eq(source_label.data.view_as(source_pred)).cpu().sum()\n",
        "\n",
        "        # 2. Target input -> Target Classification\n",
        "        target_image, target_label = target_data\n",
        "        target_image, target_label = target_image.cuda(), target_label.cuda()\n",
        "        target_feature = encoder(target_image)\n",
        "        target_output = classifier(target_feature)\n",
        "        target_pred = target_output.data.max(1, keepdim=True)[1]\n",
        "        target_correct += target_pred.eq(target_label.data.view_as(target_pred)).cpu().sum()\n",
        "\n",
        "        if training_mode == 'dann':\n",
        "            # 3. Combined input -> Domain Classificaion\n",
        "            combined_image = torch.cat((source_image, target_image), 0)  # 64 = (S:32 + T:32)\n",
        "            domain_source_labels = torch.zeros(source_label.shape[0]).type(torch.LongTensor)\n",
        "            domain_target_labels = torch.ones(target_label.shape[0]).type(torch.LongTensor)\n",
        "            domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0).cuda()\n",
        "            domain_feature = encoder(combined_image)\n",
        "            domain_output = discriminator(domain_feature, alpha)\n",
        "            domain_pred = domain_output.data.max(1, keepdim=True)[1]\n",
        "            domain_correct += domain_pred.eq(domain_combined_label.data.view_as(domain_pred)).cpu().sum()\n",
        "\n",
        "    if training_mode == 'dann':\n",
        "        print(\"Test Results on DANN :\")\n",
        "        print('\\nSource Accuracy: {}/{} ({:.2f}%)\\n'\n",
        "              'Target Accuracy: {}/{} ({:.2f}%)\\n'\n",
        "              'Domain Accuracy: {}/{} ({:.2f}%)\\n'.\n",
        "            format(\n",
        "            source_correct, len(source_test_loader.dataset), 100. * source_correct.item() / len(source_test_loader.dataset),\n",
        "            target_correct, len(target_test_loader.dataset), 100. * target_correct.item() / len(target_test_loader.dataset),\n",
        "            domain_correct, len(source_test_loader.dataset) + len(target_test_loader.dataset), 100. * domain_correct.item() / (len(source_test_loader.dataset) + len(target_test_loader.dataset))\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test results on source_only :\")\n",
        "        print('\\nSource Accuracy: {}/{} ({:.2f}%)\\n'\n",
        "              'Target Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            source_correct, len(source_test_loader.dataset), 100. * source_correct.item() / len(source_test_loader.dataset),\n",
        "            target_correct, len(target_test_loader.dataset), 100. * target_correct.item() / len(target_test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e27be3-1300-4053-88a9-7be48e3ce211",
      "metadata": {
        "id": "f7e27be3-1300-4053-88a9-7be48e3ce211"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "add9a26e-b178-4c25-830a-a097dac60477",
      "metadata": {
        "id": "add9a26e-b178-4c25-830a-a097dac60477"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Source : 0, Target :1\n",
        "source_test_loader = mnist_test_loader\n",
        "target_test_loader = mnistm_test_loader\n",
        "\n",
        "\n",
        "def source_only(encoder, classifier, source_train_loader, target_train_loader, save_name):\n",
        "    print(\"Source-only training\")\n",
        "    classifier_criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = optim.SGD(\n",
        "        list(encoder.parameters()) +\n",
        "        list(classifier.parameters()),\n",
        "        lr=0.01, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch : {}'.format(epoch))\n",
        "        set_model_mode('train', [encoder, classifier])\n",
        "\n",
        "        start_steps = epoch * len(source_train_loader)\n",
        "        total_steps = epochs * len(target_train_loader)\n",
        "\n",
        "        for batch_idx, (source_data, target_data) in enumerate(zip(source_train_loader, target_train_loader)):\n",
        "            source_image, source_label = source_data\n",
        "            p = float(batch_idx + start_steps) / total_steps\n",
        "\n",
        "            source_image = torch.cat((source_image, source_image, source_image), 1)  # MNIST convert to 3 channel\n",
        "            source_image, source_label = source_image.cuda(), source_label.cuda()  # 32\n",
        "\n",
        "            optimizer = optimizer_scheduler(optimizer=optimizer, p=p)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            source_feature = encoder(source_image)\n",
        "\n",
        "            # Classification loss\n",
        "            class_pred = classifier(source_feature)\n",
        "            class_loss = classifier_criterion(class_pred, source_label)\n",
        "\n",
        "            class_loss.backward()\n",
        "            optimizer.step()\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print('[{}/{} ({:.0f}%)]\\tClass Loss: {:.6f}'.format(batch_idx * len(source_image), len(source_train_loader.dataset), 100. * batch_idx / len(source_train_loader), class_loss.item()))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            tester(encoder, classifier, None, source_test_loader, target_test_loader, training_mode='source_only')\n",
        "    save_model(encoder, classifier, None, 'source', save_name)\n",
        "    visualize(encoder, 'source', save_name)\n",
        "\n",
        "\n",
        "def dann(encoder, classifier, discriminator, source_train_loader, target_train_loader, save_name):\n",
        "    print(\"DANN training\")\n",
        "\n",
        "    classifier_criterion = nn.CrossEntropyLoss().cuda()\n",
        "    discriminator_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "    list(encoder.parameters()) +\n",
        "    list(classifier.parameters()) +\n",
        "    list(discriminator.parameters()),\n",
        "    lr=0.01,\n",
        "    momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch : {}'.format(epoch))\n",
        "        set_model_mode('train', [encoder, classifier, discriminator])\n",
        "\n",
        "        start_steps = epoch * len(source_train_loader)\n",
        "        total_steps = epochs * len(target_train_loader)\n",
        "\n",
        "        for batch_idx, (source_data, target_data) in enumerate(zip(source_train_loader, target_train_loader)):\n",
        "\n",
        "            source_image, source_label = source_data\n",
        "            target_image, target_label = target_data\n",
        "\n",
        "            p = float(batch_idx + start_steps) / total_steps\n",
        "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "            source_image = torch.cat((source_image, source_image, source_image), 1)\n",
        "\n",
        "            source_image, source_label = source_image.cuda(), source_label.cuda()\n",
        "            target_image, target_label = target_image.cuda(), target_label.cuda()\n",
        "            combined_image = torch.cat((source_image, target_image), 0)\n",
        "\n",
        "            optimizer = optimizer_scheduler(optimizer=optimizer, p=p)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            combined_feature = encoder(combined_image)\n",
        "            source_feature = encoder(source_image)\n",
        "\n",
        "            # 1.Classification loss\n",
        "            class_pred = classifier(source_feature)\n",
        "            class_loss = classifier_criterion(class_pred, source_label)\n",
        "\n",
        "            # 2. Domain loss\n",
        "            domain_pred = discriminator(combined_feature, alpha)\n",
        "\n",
        "            domain_source_labels = torch.zeros(source_label.shape[0]).type(torch.LongTensor)\n",
        "            domain_target_labels = torch.ones(target_label.shape[0]).type(torch.LongTensor)\n",
        "            domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0).cuda()\n",
        "            domain_loss = discriminator_criterion(domain_pred, domain_combined_label)\n",
        "\n",
        "            total_loss = class_loss + domain_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print('[{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tClass Loss: {:.6f}\\tDomain Loss: {:.6f}'.format(\n",
        "                    batch_idx * len(target_image), len(target_train_loader.dataset), 100. * batch_idx / len(target_train_loader), total_loss.item(), class_loss.item(), domain_loss.item()))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode='dann')\n",
        "\n",
        "    save_model(encoder, classifier, discriminator, 'source', save_name)\n",
        "    visualize(encoder, 'source', save_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e269390-5906-43b7-97dc-7e7296d3e3d7",
      "metadata": {
        "id": "6e269390-5906-43b7-97dc-7e7296d3e3d7"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8a87d5c5-cfd8-4168-b170-a2c5293546be",
      "metadata": {
        "id": "8a87d5c5-cfd8-4168-b170-a2c5293546be",
        "outputId": "9be541a1-1557-483a-a8bf-6d7696beb43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GPU : 0\n",
            "Source-only training\n",
            "Epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-5ea6f2a6f2c3>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1568/60000 (3%)]\tClass Loss: 2.302814\n",
            "[3168/60000 (6%)]\tClass Loss: 2.301854\n",
            "[4768/60000 (9%)]\tClass Loss: 2.300783\n",
            "[6368/60000 (12%)]\tClass Loss: 2.299937\n",
            "[7968/60000 (14%)]\tClass Loss: 2.296428\n",
            "[9568/60000 (17%)]\tClass Loss: 2.293837\n",
            "[11168/60000 (20%)]\tClass Loss: 2.290826\n",
            "[12768/60000 (23%)]\tClass Loss: 2.288547\n",
            "[14368/60000 (26%)]\tClass Loss: 2.158783\n",
            "[15968/60000 (29%)]\tClass Loss: 1.895252\n",
            "[17568/60000 (32%)]\tClass Loss: 1.958928\n",
            "[19168/60000 (35%)]\tClass Loss: 1.828215\n",
            "[20768/60000 (38%)]\tClass Loss: 1.691538\n",
            "[22368/60000 (41%)]\tClass Loss: 1.706920\n",
            "[23968/60000 (44%)]\tClass Loss: 1.680334\n",
            "[25568/60000 (46%)]\tClass Loss: 1.592805\n",
            "[27168/60000 (49%)]\tClass Loss: 1.573833\n",
            "[28768/60000 (52%)]\tClass Loss: 1.651500\n",
            "[30368/60000 (55%)]\tClass Loss: 1.529578\n",
            "[31968/60000 (58%)]\tClass Loss: 1.591294\n",
            "[33568/60000 (61%)]\tClass Loss: 1.626834\n",
            "[35168/60000 (64%)]\tClass Loss: 1.647551\n",
            "[36768/60000 (67%)]\tClass Loss: 1.586869\n",
            "[38368/60000 (70%)]\tClass Loss: 1.656207\n",
            "[39968/60000 (73%)]\tClass Loss: 1.530580\n",
            "[41568/60000 (76%)]\tClass Loss: 1.586759\n",
            "[43168/60000 (78%)]\tClass Loss: 1.585163\n",
            "[44768/60000 (81%)]\tClass Loss: 1.645041\n",
            "[46368/60000 (84%)]\tClass Loss: 1.574695\n",
            "[47968/60000 (87%)]\tClass Loss: 1.618115\n",
            "[49568/60000 (90%)]\tClass Loss: 1.585720\n",
            "[51168/60000 (93%)]\tClass Loss: 1.486995\n",
            "[52768/60000 (96%)]\tClass Loss: 1.513495\n",
            "[54368/60000 (99%)]\tClass Loss: 1.528770\n",
            "Epoch : 1\n",
            "[1568/60000 (3%)]\tClass Loss: 1.564070\n",
            "[3168/60000 (6%)]\tClass Loss: 1.492704\n",
            "[4768/60000 (9%)]\tClass Loss: 1.462630\n",
            "[6368/60000 (12%)]\tClass Loss: 1.526002\n",
            "[7968/60000 (14%)]\tClass Loss: 1.487579\n",
            "[9568/60000 (17%)]\tClass Loss: 1.525842\n",
            "[11168/60000 (20%)]\tClass Loss: 1.464265\n",
            "[12768/60000 (23%)]\tClass Loss: 1.469298\n",
            "[14368/60000 (26%)]\tClass Loss: 1.519402\n",
            "[15968/60000 (29%)]\tClass Loss: 1.520618\n",
            "[17568/60000 (32%)]\tClass Loss: 1.486383\n",
            "[19168/60000 (35%)]\tClass Loss: 1.522857\n",
            "[20768/60000 (38%)]\tClass Loss: 1.481926\n",
            "[22368/60000 (41%)]\tClass Loss: 1.492148\n",
            "[23968/60000 (44%)]\tClass Loss: 1.510821\n",
            "[25568/60000 (46%)]\tClass Loss: 1.462250\n",
            "[27168/60000 (49%)]\tClass Loss: 1.474334\n",
            "[28768/60000 (52%)]\tClass Loss: 1.487918\n",
            "[30368/60000 (55%)]\tClass Loss: 1.556226\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461495\n",
            "[33568/60000 (61%)]\tClass Loss: 1.553095\n",
            "[35168/60000 (64%)]\tClass Loss: 1.487763\n",
            "[36768/60000 (67%)]\tClass Loss: 1.461482\n",
            "[38368/60000 (70%)]\tClass Loss: 1.552446\n",
            "[39968/60000 (73%)]\tClass Loss: 1.492335\n",
            "[41568/60000 (76%)]\tClass Loss: 1.471534\n",
            "[43168/60000 (78%)]\tClass Loss: 1.485507\n",
            "[44768/60000 (81%)]\tClass Loss: 1.495046\n",
            "[46368/60000 (84%)]\tClass Loss: 1.462307\n",
            "[47968/60000 (87%)]\tClass Loss: 1.492404\n",
            "[49568/60000 (90%)]\tClass Loss: 1.468614\n",
            "[51168/60000 (93%)]\tClass Loss: 1.461276\n",
            "[52768/60000 (96%)]\tClass Loss: 1.464119\n",
            "[54368/60000 (99%)]\tClass Loss: 1.461975\n",
            "Epoch : 2\n",
            "[1568/60000 (3%)]\tClass Loss: 1.462730\n",
            "[3168/60000 (6%)]\tClass Loss: 1.461191\n",
            "[4768/60000 (9%)]\tClass Loss: 1.489359\n",
            "[6368/60000 (12%)]\tClass Loss: 1.526161\n",
            "[7968/60000 (14%)]\tClass Loss: 1.478691\n",
            "[9568/60000 (17%)]\tClass Loss: 1.500520\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461198\n",
            "[12768/60000 (23%)]\tClass Loss: 1.492499\n",
            "[14368/60000 (26%)]\tClass Loss: 1.490966\n",
            "[15968/60000 (29%)]\tClass Loss: 1.479718\n",
            "[17568/60000 (32%)]\tClass Loss: 1.530256\n",
            "[19168/60000 (35%)]\tClass Loss: 1.461195\n",
            "[20768/60000 (38%)]\tClass Loss: 1.495237\n",
            "[22368/60000 (41%)]\tClass Loss: 1.499987\n",
            "[23968/60000 (44%)]\tClass Loss: 1.461233\n",
            "[25568/60000 (46%)]\tClass Loss: 1.462958\n",
            "[27168/60000 (49%)]\tClass Loss: 1.466536\n",
            "[28768/60000 (52%)]\tClass Loss: 1.467357\n",
            "[30368/60000 (55%)]\tClass Loss: 1.466746\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461549\n",
            "[33568/60000 (61%)]\tClass Loss: 1.465632\n",
            "[35168/60000 (64%)]\tClass Loss: 1.495515\n",
            "[36768/60000 (67%)]\tClass Loss: 1.495049\n",
            "[38368/60000 (70%)]\tClass Loss: 1.567507\n",
            "[39968/60000 (73%)]\tClass Loss: 1.464036\n",
            "[41568/60000 (76%)]\tClass Loss: 1.497964\n",
            "[43168/60000 (78%)]\tClass Loss: 1.461284\n",
            "[44768/60000 (81%)]\tClass Loss: 1.485718\n",
            "[46368/60000 (84%)]\tClass Loss: 1.476040\n",
            "[47968/60000 (87%)]\tClass Loss: 1.461273\n",
            "[49568/60000 (90%)]\tClass Loss: 1.461921\n",
            "[51168/60000 (93%)]\tClass Loss: 1.508619\n",
            "[52768/60000 (96%)]\tClass Loss: 1.517552\n",
            "[54368/60000 (99%)]\tClass Loss: 1.494063\n",
            "Epoch : 3\n",
            "[1568/60000 (3%)]\tClass Loss: 1.463914\n",
            "[3168/60000 (6%)]\tClass Loss: 1.522352\n",
            "[4768/60000 (9%)]\tClass Loss: 1.492537\n",
            "[6368/60000 (12%)]\tClass Loss: 1.491403\n",
            "[7968/60000 (14%)]\tClass Loss: 1.469553\n",
            "[9568/60000 (17%)]\tClass Loss: 1.461160\n",
            "[11168/60000 (20%)]\tClass Loss: 1.472609\n",
            "[12768/60000 (23%)]\tClass Loss: 1.525506\n",
            "[14368/60000 (26%)]\tClass Loss: 1.461599\n",
            "[15968/60000 (29%)]\tClass Loss: 1.481465\n",
            "[17568/60000 (32%)]\tClass Loss: 1.463036\n",
            "[19168/60000 (35%)]\tClass Loss: 1.522766\n",
            "[20768/60000 (38%)]\tClass Loss: 1.492564\n",
            "[22368/60000 (41%)]\tClass Loss: 1.481448\n",
            "[23968/60000 (44%)]\tClass Loss: 1.490673\n",
            "[25568/60000 (46%)]\tClass Loss: 1.492768\n",
            "[27168/60000 (49%)]\tClass Loss: 1.472413\n",
            "[28768/60000 (52%)]\tClass Loss: 1.492518\n",
            "[30368/60000 (55%)]\tClass Loss: 1.461161\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461158\n",
            "[33568/60000 (61%)]\tClass Loss: 1.473462\n",
            "[35168/60000 (64%)]\tClass Loss: 1.490730\n",
            "[36768/60000 (67%)]\tClass Loss: 1.583853\n",
            "[38368/60000 (70%)]\tClass Loss: 1.462110\n",
            "[39968/60000 (73%)]\tClass Loss: 1.486297\n",
            "[41568/60000 (76%)]\tClass Loss: 1.483710\n",
            "[43168/60000 (78%)]\tClass Loss: 1.507705\n",
            "[44768/60000 (81%)]\tClass Loss: 1.466230\n",
            "[46368/60000 (84%)]\tClass Loss: 1.461619\n",
            "[47968/60000 (87%)]\tClass Loss: 1.476074\n",
            "[49568/60000 (90%)]\tClass Loss: 1.471305\n",
            "[51168/60000 (93%)]\tClass Loss: 1.461255\n",
            "[52768/60000 (96%)]\tClass Loss: 1.466076\n",
            "[54368/60000 (99%)]\tClass Loss: 1.492462\n",
            "Epoch : 4\n",
            "[1568/60000 (3%)]\tClass Loss: 1.492404\n",
            "[3168/60000 (6%)]\tClass Loss: 1.461310\n",
            "[4768/60000 (9%)]\tClass Loss: 1.492398\n",
            "[6368/60000 (12%)]\tClass Loss: 1.461550\n",
            "[7968/60000 (14%)]\tClass Loss: 1.493024\n",
            "[9568/60000 (17%)]\tClass Loss: 1.461155\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461152\n",
            "[12768/60000 (23%)]\tClass Loss: 1.492243\n",
            "[14368/60000 (26%)]\tClass Loss: 1.461177\n",
            "[15968/60000 (29%)]\tClass Loss: 1.461176\n",
            "[17568/60000 (32%)]\tClass Loss: 1.490455\n",
            "[19168/60000 (35%)]\tClass Loss: 1.510408\n",
            "[20768/60000 (38%)]\tClass Loss: 1.463985\n",
            "[22368/60000 (41%)]\tClass Loss: 1.488748\n",
            "[23968/60000 (44%)]\tClass Loss: 1.461889\n",
            "[25568/60000 (46%)]\tClass Loss: 1.461925\n",
            "[27168/60000 (49%)]\tClass Loss: 1.462271\n",
            "[28768/60000 (52%)]\tClass Loss: 1.481773\n",
            "[30368/60000 (55%)]\tClass Loss: 1.519071\n",
            "[31968/60000 (58%)]\tClass Loss: 1.494598\n",
            "[33568/60000 (61%)]\tClass Loss: 1.461167\n",
            "[35168/60000 (64%)]\tClass Loss: 1.461177\n",
            "[36768/60000 (67%)]\tClass Loss: 1.461730\n",
            "[38368/60000 (70%)]\tClass Loss: 1.500912\n",
            "[39968/60000 (73%)]\tClass Loss: 1.496094\n",
            "[41568/60000 (76%)]\tClass Loss: 1.546242\n",
            "[43168/60000 (78%)]\tClass Loss: 1.461284\n",
            "[44768/60000 (81%)]\tClass Loss: 1.492335\n",
            "[46368/60000 (84%)]\tClass Loss: 1.476663\n",
            "[47968/60000 (87%)]\tClass Loss: 1.461151\n",
            "[49568/60000 (90%)]\tClass Loss: 1.481650\n",
            "[51168/60000 (93%)]\tClass Loss: 1.461151\n",
            "[52768/60000 (96%)]\tClass Loss: 1.483651\n",
            "[54368/60000 (99%)]\tClass Loss: 1.491682\n",
            "Epoch : 5\n",
            "[1568/60000 (3%)]\tClass Loss: 1.489815\n",
            "[3168/60000 (6%)]\tClass Loss: 1.461563\n",
            "[4768/60000 (9%)]\tClass Loss: 1.465755\n",
            "[6368/60000 (12%)]\tClass Loss: 1.465776\n",
            "[7968/60000 (14%)]\tClass Loss: 1.461151\n",
            "[9568/60000 (17%)]\tClass Loss: 1.506725\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461397\n",
            "[12768/60000 (23%)]\tClass Loss: 1.461432\n",
            "[14368/60000 (26%)]\tClass Loss: 1.461153\n",
            "[15968/60000 (29%)]\tClass Loss: 1.488016\n",
            "[17568/60000 (32%)]\tClass Loss: 1.507422\n",
            "[19168/60000 (35%)]\tClass Loss: 1.461155\n",
            "[20768/60000 (38%)]\tClass Loss: 1.461543\n",
            "[22368/60000 (41%)]\tClass Loss: 1.462945\n",
            "[23968/60000 (44%)]\tClass Loss: 1.461158\n",
            "[25568/60000 (46%)]\tClass Loss: 1.461380\n",
            "[27168/60000 (49%)]\tClass Loss: 1.461247\n",
            "[28768/60000 (52%)]\tClass Loss: 1.461153\n",
            "[30368/60000 (55%)]\tClass Loss: 1.462972\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461189\n",
            "[33568/60000 (61%)]\tClass Loss: 1.461691\n",
            "[35168/60000 (64%)]\tClass Loss: 1.518159\n",
            "[36768/60000 (67%)]\tClass Loss: 1.492312\n",
            "[38368/60000 (70%)]\tClass Loss: 1.485003\n",
            "[39968/60000 (73%)]\tClass Loss: 1.469618\n",
            "[41568/60000 (76%)]\tClass Loss: 1.461153\n",
            "[43168/60000 (78%)]\tClass Loss: 1.489318\n",
            "[44768/60000 (81%)]\tClass Loss: 1.461165\n",
            "[46368/60000 (84%)]\tClass Loss: 1.471815\n",
            "[47968/60000 (87%)]\tClass Loss: 1.461155\n",
            "[49568/60000 (90%)]\tClass Loss: 1.461165\n",
            "[51168/60000 (93%)]\tClass Loss: 1.480294\n",
            "[52768/60000 (96%)]\tClass Loss: 1.484388\n",
            "[54368/60000 (99%)]\tClass Loss: 1.461434\n",
            "Epoch : 6\n",
            "[1568/60000 (3%)]\tClass Loss: 1.490156\n",
            "[3168/60000 (6%)]\tClass Loss: 1.491632\n",
            "[4768/60000 (9%)]\tClass Loss: 1.461151\n",
            "[6368/60000 (12%)]\tClass Loss: 1.468633\n",
            "[7968/60000 (14%)]\tClass Loss: 1.498486\n",
            "[9568/60000 (17%)]\tClass Loss: 1.461749\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461154\n",
            "[12768/60000 (23%)]\tClass Loss: 1.489158\n",
            "[14368/60000 (26%)]\tClass Loss: 1.476182\n",
            "[15968/60000 (29%)]\tClass Loss: 1.469709\n",
            "[17568/60000 (32%)]\tClass Loss: 1.493902\n",
            "[19168/60000 (35%)]\tClass Loss: 1.462390\n",
            "[20768/60000 (38%)]\tClass Loss: 1.461155\n",
            "[22368/60000 (41%)]\tClass Loss: 1.461161\n",
            "[23968/60000 (44%)]\tClass Loss: 1.461151\n",
            "[25568/60000 (46%)]\tClass Loss: 1.461164\n",
            "[27168/60000 (49%)]\tClass Loss: 1.461216\n",
            "[28768/60000 (52%)]\tClass Loss: 1.461152\n",
            "[30368/60000 (55%)]\tClass Loss: 1.461173\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461516\n",
            "[33568/60000 (61%)]\tClass Loss: 1.464752\n",
            "[35168/60000 (64%)]\tClass Loss: 1.492454\n",
            "[36768/60000 (67%)]\tClass Loss: 1.492832\n",
            "[38368/60000 (70%)]\tClass Loss: 1.461244\n",
            "[39968/60000 (73%)]\tClass Loss: 1.461211\n",
            "[41568/60000 (76%)]\tClass Loss: 1.492271\n",
            "[43168/60000 (78%)]\tClass Loss: 1.505684\n",
            "[44768/60000 (81%)]\tClass Loss: 1.472543\n",
            "[46368/60000 (84%)]\tClass Loss: 1.461153\n",
            "[47968/60000 (87%)]\tClass Loss: 1.492348\n",
            "[49568/60000 (90%)]\tClass Loss: 1.461151\n",
            "[51168/60000 (93%)]\tClass Loss: 1.461164\n",
            "[52768/60000 (96%)]\tClass Loss: 1.487974\n",
            "[54368/60000 (99%)]\tClass Loss: 1.462402\n",
            "Epoch : 7\n",
            "[1568/60000 (3%)]\tClass Loss: 1.466301\n",
            "[3168/60000 (6%)]\tClass Loss: 1.461263\n",
            "[4768/60000 (9%)]\tClass Loss: 1.462711\n",
            "[6368/60000 (12%)]\tClass Loss: 1.461156\n",
            "[7968/60000 (14%)]\tClass Loss: 1.461167\n",
            "[9568/60000 (17%)]\tClass Loss: 1.500032\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461155\n",
            "[12768/60000 (23%)]\tClass Loss: 1.461151\n",
            "[14368/60000 (26%)]\tClass Loss: 1.461521\n",
            "[15968/60000 (29%)]\tClass Loss: 1.461151\n",
            "[17568/60000 (32%)]\tClass Loss: 1.464688\n",
            "[19168/60000 (35%)]\tClass Loss: 1.461160\n",
            "[20768/60000 (38%)]\tClass Loss: 1.461151\n",
            "[22368/60000 (41%)]\tClass Loss: 1.522505\n",
            "[23968/60000 (44%)]\tClass Loss: 1.461160\n",
            "[25568/60000 (46%)]\tClass Loss: 1.465264\n",
            "[27168/60000 (49%)]\tClass Loss: 1.461151\n",
            "[28768/60000 (52%)]\tClass Loss: 1.492435\n",
            "[30368/60000 (55%)]\tClass Loss: 1.463315\n",
            "[31968/60000 (58%)]\tClass Loss: 1.489402\n",
            "[33568/60000 (61%)]\tClass Loss: 1.461151\n",
            "[35168/60000 (64%)]\tClass Loss: 1.467955\n",
            "[36768/60000 (67%)]\tClass Loss: 1.461313\n",
            "[38368/60000 (70%)]\tClass Loss: 1.461151\n",
            "[39968/60000 (73%)]\tClass Loss: 1.461166\n",
            "[41568/60000 (76%)]\tClass Loss: 1.486620\n",
            "[43168/60000 (78%)]\tClass Loss: 1.492390\n",
            "[44768/60000 (81%)]\tClass Loss: 1.481594\n",
            "[46368/60000 (84%)]\tClass Loss: 1.461151\n",
            "[47968/60000 (87%)]\tClass Loss: 1.490872\n",
            "[49568/60000 (90%)]\tClass Loss: 1.488724\n",
            "[51168/60000 (93%)]\tClass Loss: 1.461155\n",
            "[52768/60000 (96%)]\tClass Loss: 1.461481\n",
            "[54368/60000 (99%)]\tClass Loss: 1.494047\n",
            "Epoch : 8\n",
            "[1568/60000 (3%)]\tClass Loss: 1.461154\n",
            "[3168/60000 (6%)]\tClass Loss: 1.461159\n",
            "[4768/60000 (9%)]\tClass Loss: 1.461152\n",
            "[6368/60000 (12%)]\tClass Loss: 1.501829\n",
            "[7968/60000 (14%)]\tClass Loss: 1.461308\n",
            "[9568/60000 (17%)]\tClass Loss: 1.461156\n",
            "[11168/60000 (20%)]\tClass Loss: 1.461170\n",
            "[12768/60000 (23%)]\tClass Loss: 1.475383\n",
            "[14368/60000 (26%)]\tClass Loss: 1.461587\n",
            "[15968/60000 (29%)]\tClass Loss: 1.478827\n",
            "[17568/60000 (32%)]\tClass Loss: 1.461249\n",
            "[19168/60000 (35%)]\tClass Loss: 1.461151\n",
            "[20768/60000 (38%)]\tClass Loss: 1.467095\n",
            "[22368/60000 (41%)]\tClass Loss: 1.461167\n",
            "[23968/60000 (44%)]\tClass Loss: 1.554286\n",
            "[25568/60000 (46%)]\tClass Loss: 1.461155\n",
            "[27168/60000 (49%)]\tClass Loss: 1.461151\n",
            "[28768/60000 (52%)]\tClass Loss: 1.461154\n",
            "[30368/60000 (55%)]\tClass Loss: 1.461190\n",
            "[31968/60000 (58%)]\tClass Loss: 1.461154\n",
            "[33568/60000 (61%)]\tClass Loss: 1.461231\n",
            "[35168/60000 (64%)]\tClass Loss: 1.491793\n",
            "[36768/60000 (67%)]\tClass Loss: 1.461151\n",
            "[38368/60000 (70%)]\tClass Loss: 1.464761\n",
            "[39968/60000 (73%)]\tClass Loss: 1.491259\n",
            "[41568/60000 (76%)]\tClass Loss: 1.461549\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-63c521e8c30d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-63c521e8c30d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msource_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-c689d28e994e>\u001b[0m in \u001b[0;36msource_only\u001b[0;34m(encoder, classifier, source_train_loader, target_train_loader, save_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0msource_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "save_name = 'omg'\n",
        "\n",
        "def main():\n",
        "    source_train_loader = mnist_train_loader\n",
        "    target_train_loader = mnistm_train_loader\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        get_free_gpu()\n",
        "        print('Running GPU : {}'.format(torch.cuda.current_device()))\n",
        "        encoder = Extractor().cuda()\n",
        "        classifier = Classifier().cuda()\n",
        "        discriminator = Discriminator().cuda()\n",
        "\n",
        "        source_only(encoder, classifier, source_train_loader, target_train_loader, save_name)\n",
        "        dann(encoder, classifier, discriminator, source_train_loader, target_train_loader, save_name)\n",
        "\n",
        "    else:\n",
        "        print(\"There is no GPU -_-!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model()"
      ],
      "metadata": {
        "id": "PaQL3-3k5bxx"
      },
      "id": "PaQL3-3k5bxx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}